<!DOCTYPE HTML>
<html lang="en">
  <head>
    <title>Jingxiang Guo</title>
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Jingxiang Guo">
    <meta name="viewport" content="width=900px, initial-scale=0.8, maximum-scale=2.0, user-scalable=yes">
    <meta name="google-site-verification" content="ky_GtW3ix1x6MhH-pM6PonkpJCmE-237rRUvxvyQjVs" />

    <link rel="shortcut icon" href="images/favicon.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    <!-- MathJax -->
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>        
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      });
    </script>

    <!-- visitor statistics -->
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <style>
      p {
        margin-top: 10px;
        margin-bottom: 10px;
        line-height: 1.6;
      }
      .TLDR {
        margin-top: 8px;
        background-color: #f9f9f9;
        padding: 8px;
        border-left: 3px solid #66c0ff;
      }
      .author {
        font-size: 16px;
      }
      .strong-author {
        font-size: 16px;
        font-weight: bold;
      }
      .academic-links {
        display: none;
        position: fixed;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        background: white;
        padding: 20px;
        border-radius: 5px;
        box-shadow: 0 0 10px rgba(0,0,0,0.3);
        z-index: 1000;
      }
    </style>
  </head>

  <body>
    <table style="width:100%; max-width:900px; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Introduction & Profile Photo -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr style="padding:0px">

                  <!-- introduction -->
                  <td style="padding: 20px; width:64%; vertical-align:middle">
                    <p class="webname" style="text-align: center;">
                      Jingxiang Guo &nbsp;|&nbsp; ÈÉ≠‰∫¨Áøî
                    </p>

                    <p>
                      I am a fourth-year undergraduate student majoring in Automation at <a href="http://en.hitsz.edu.cn/">Harbin Institute of Technology, Shenzhen</a>. 
                      Currently, I am fortune to be a research assistant at <a href="https://scalelab-sjtu.github.io/">SJTU ScaleLab</a> advised by Prof. <a href="https://yaomarkmu.github.io/">Yao Mu</a>. Previously, I was an intern at <a href="https://nus.edu.sg/">NUS LinS Lab</a> advised by Prof. 
                      <a href="https://linsats.github.io/">Lin Shao</a> and <a href="http://reinforcement.hitsz.edu.cn/index.html">HITsz RLGroup</a> advised by Prof. <a href="https://faculty.hitsz.edu.cn/liyanjie">Yanjie Li</a>.
                    </p>

                    <p style="text-align:center; margin-top: 0px; margin-bottom: 0px;">
                      <a href="mailto:jingxiangguo@u.nus.edu" class="btn-sm badge-button">Email</a> &nbsp;/&nbsp;
                      <a href="data/CV.pdf" class="btn-sm badge-button">CV</a> &nbsp;/&nbsp;  
                      <a href="https://github.com/BorisGuo6" class="btn-sm badge-button">GitHub</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=JEjiaq0AAAAJ" class="btn-sm badge-button">Google Scholar</a> &nbsp;/&nbsp;
                      <a href="javascript:void(0)" onclick="openAcademicLinks()" class="btn-sm badge-button">More Academic Links</a> &nbsp;/&nbsp;
                      <a href="javascript:void(0)" onclick="openWeChatModal()" class="btn-sm badge-button">WeChat</a>
                    </p>

                    <!-- Academic Links Modal -->
                    <div id="AcademicLinksModal" class="academic-links">
                      <span class="close" onclick="closeAcademicLinks()">&times;</span>
                      <h3>Academic Links</h3>
                      <p>
                        <a href="https://ieeexplore.ieee.org/author/37089341227">IEEE Xplore</a><br>
                        <a href="https://www.semanticscholar.org/author/Jingxiang-Guo/2307562741">Semantic Scholar</a><br>
                        <a href="https://dblp.org/pid/386/2489.html">dblp</a><br>
                        <a href="https://www.scopus.com/authid/detail.uri?authorId=59210015500">Scopus</a><br>
                        <a href="https://www.researchgate.net/profile/Jingxiang-Guo-2">ResearchGate</a><br>
                        <a href="https://www.linkedin.com/in/borisguo">LinkedIn</a><br>
                        <a href="https://borisguo.link/">Gravatar</a><br>
                        <a href="https://orcid.org/my-orcid?orcid=0009-0009-2314-6911">ORCID</a>
                      </p>
                    </div>

                    <script>
                      function openAcademicLinks() {
                        document.getElementById("AcademicLinksModal").style.display = "block";
                      }
                      function closeAcademicLinks() {
                        document.getElementById("AcademicLinksModal").style.display = "none";
                      }
                      window.onclick = function(event) {
                        var modal = document.getElementById("AcademicLinksModal");
                        if (event.target == modal) {
                          modal.style.display = "none";
                        }
                      }
                    </script>

                    <!-- WeChat image function -->
                    <div id="WeChatModal" class="modal">
                      <div class="modal-content">
                        <span class="close" onclick="closeWeChatModal()">&times;</span>
                        <img src="images/WeChat.jpg" alt="WeChat QR Code" style="width: 100%;">
                      </div>
                    </div>
                    
                    <script>
                      function openWeChatModal() {
                        document.getElementById("WeChatModal").style.display = "block";
                      }
                      function closeWeChatModal() {
                        document.getElementById("WeChatModal").style.display = "none";
                      }
                      window.onload = function() {
                        document.getElementById("WeChatModal").style.display = "none";
                      };
                      window.onclick = function(event) {
                        var modal = document.getElementById("WeChatModal");
                        if (event.target == modal) {
                          modal.style.display = "none";
                        }
                      }
                    </script>
                  </td>

                  <!-- profile photo -->
                  <td style="padding: 20px; padding-left: 0px; padding-bottom: 0px; width:20%; max-width:40%">
                    <img 
                    style="width:100%; max-width:100%; object-fit: cover; border-radius: 0%; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);" 
                    alt="profile photo" 
                    src="images/jingxiangguo.jpg" 
                    class="hoverZoomLink">
                  </td>
                </tr>
              </tbody>
            </table>
            
            <!-- News -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 0px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>News</b></h2>
                    <ul class="news-list">
                      <li> [2025/05] üèÖ <a href="https://nus-lins-lab.github.io/drograspweb/">$\mathcal{D(R, O)}$ Grasp</a> won the <strong style="color: red;">ICRA 2025 Best Paper Award on Robot Manipulation and Locomotion</strong>,
                        and <a href="https://nus-lins-lab.github.io/telepreview-web/">TelePreview</a> won the <strong style="color: red;">Best Paper Award</strong> at ICRA 2025 Workshop on Human-Centric Multilateral Teleoperation! </li>
                      <li> [2025/04] üèÖ <a href="https://nus-lins-lab.github.io/drograspweb/">$\mathcal{D(R, O)}$ Grasp</a> has been selected as an <strong style="color: red;">ICRA 2025 Best Paper Award Finalist</strong>! </li>
                      <li>
                        [2025/04] üéâ Manual2Skill was accepted to RSS 2025.
                      </li>
                      <li>
                        [2024/12] üéâ $\mathcal{D(R, O)}$ Grasp was accepted to ICRA 2025.
                      </li>
                      <li>
                        [2024/11] üèÖ <a href="https://nus-lins-lab.github.io/drograspweb/"><b>$\mathcal{D(R, O)}$ Grasp</b></a> won the 
                      <strong style="color: rgba(255, 69, 58, 1);">Best Robotics Paper Award</strong> at <a href="https://sites.google.com/view/corl-mapodel-workshop">CoRL 2024 MAPoDeL Workshop</a>! 
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Interest -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 0px; padding-bottom: 0px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>Research</b></h2>
                    <p style="margin-top: 10px; margin-bottom: 5px;">
                      My research interests lie in ü§ñ robot learning, ü¶æ dexterous manipulation, and ü§ù Human-Robot Perception Alignment. 
                      My long-term goal is to create true robotic life, pushing the boundaries of what's possible with machines. 
                      I'm open to collaborations on robotics-related projects! Whether you're a researcher looking for a partner, 
                      feel free to reach out to meüëã @ <!-- Calendly link widget begin -->
                      <link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet">
                      <script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
                      <a href="" onclick="Calendly.initPopupWidget({url: 'https://calendly.com/borisguo6/30min'});return false;">Schedule time with me</a>
                      <!-- Calendly link widget end -->.
                    </p>
                    <p style="margin-top: 0px; margin-bottom: 10px;">
                      Papers sorted by recency. Representative papers are <span style="background-color: rgba(102, 192, 255, 0.2);">highlighted</span>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Paper List -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <!-- World4Omni -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/world4omni.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);"> 
                  </td>
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation
                    </span>
                    <br>
                    <i>
                      <span class="author">Haonan Chen*</span>,
                      <span class="author">Bangjun Wang*</span>,
                      <span class="strong-author">Jingxiang Guo*</span>,
                      <span class="author">Tianrui Zhang</span>,
                      <span class="author">Yiwen Hou</span>,
                      <span class="author">Xuchuan Huang</span>,
                      <span class="author">Chenrui Tie</span>,
                      <span class="author">Lin Shao</span>
                    </i>
                    <br>
                    <em>In submission</em>
                    <br>
                    <a href="https://world4omni.github.io/">Website</a>
                    &nbsp;/&nbsp;
                    arXiv
                    &nbsp;/&nbsp;
                    Code
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Propose a novel framework that leverages a pre-trained multimodal image-generation model as a world model to guide policy learning.
                    </div>
                  </td>
                </tr>

                <!-- DexSing -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/dexsing.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      DexSinGrasp: Learning a Unified Policy for Dexterous Object Singulation and Grasping in Cluttered Environments
                    </span>
                    <br>
                
                    <i>
                      <span class="author">Lixin Xu</span>,
                      <span class="author">Zixuan Liu</span>,
                      <span class="author">Zhewei Gui</span>,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Zeyu Jiang</span>,
                      <span class="author">Zhixuan Xu</span>,
                      <span class="author">Chongkai Gao</span>,
                      <span class="author">Lin Shao</span>
                    </i>
                    <br>

                    <em>In submission to IROS</em>
                    <br>
                    <span class="strong-venue">
                      <i class="fa-solid fa-star"></i> Spotlight Presentation, ICRA 2025
                      <a href="https://sites.google.com/view/dexterity-workshop-icra2025/home" style="color: #66c0ff">@ Handy Moves: Dexterity in Multi-Fingered Hands </a><i class="fa-solid fa-star"></i>
                    </span>
                    <br>
    
                    <a href="https://nus-lins-lab.github.io/dexsingweb/">Website</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2504.04516">arXiv</a>
                    &nbsp;/&nbsp;
                    Code

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Implement a unified policy for dexterous object singulation and grasping in cluttered environments, 
                      enabling robots to handle complex manipulation tasks with high success rates.
                    </div>
                  </td>
                </tr>

                <!-- Manual -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/Manual.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models
                    </span>
                    <br>
                
                    <i>
                      <span class="author">Chenrui Tie</span>,
                      <span class="author">Shengxiang Sun</span>,
                      <span class="author">Jinxuan Zhu</span>,
                      <span class="author">Yiwei Liu</span>,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Yue Hu</span>,
                      <span class="author">Haonan Chen</span>,
                      <span class="author">Junting Chen</span>,
                      <span class="author">Ruihai Wu</span>,
                      <span class="author">Lin Shao</span>
                    </i>
                    <br>
                    <!-- RSS 2025 -->
                    <span class="accepted-venue">RSS 2025</span>&nbsp;
                    <span class="accepted-venue-detail">Robotics: Science and Systems</span>
                    <br>
                    <span class="strong-venue">
                      <i class="fa-solid fa-star"></i> Oral Presentation, CVPR 2025
                      <a href="https://robo-3dvlms.github.io/" style="color: #66c0ff">@ 3D Vision Language Models for Robotic Manipulation</a><i class="fa-solid fa-star"></i>
                    </span>
                    <br>
                    <a href="https://owensun2004.github.io/Furniture-Assembly-Web/">Website</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2502.10090">arXiv</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/owensun2004/Manual2Skill">Code</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Propose a novel approach that leverages vision-language models to interpret assembly manuals and 
                      translate them into executable robotic skills for furniture assembly tasks.
                    </div>
                  </td>
                </tr>

                <!-- TelePreview -->
                <tr style="background-color: rgba(102, 192, 255, 0.15);">
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/TelePreview.gif' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      TelePreview: A User-Friendly Teleoperation System with Virtual Arm Assistance for Enhanced Effectiveness
                    </span>
                    <br>

                    <i>
                      <span class="strong-author">Jingxiang Guo</span>*,
                      <span class="author">Jiayu Luo</span>*,
                      <span class="author">Zhenyu Wei</span>*,
                      <span class="author">Yiwen Hou</span>,
                      <span class="author">Zhixuan Xu</span>,
                      <span class="author">Xiaoyi Lin</span>,
                      <span class="author">Chongkai Gao</span>,
                      <span class="author">Lin Shao</span>
                    </i>
                    <br>
                    
                    <a href="https://nus-lins-lab.github.io/telepreview-web/">Website</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2412.13548">arXiv</a>
                    &nbsp;/&nbsp;
                    Code (Coming soon)
                    <br>

                    <div>
                      <span class="award-venue">
                        <i class="fa-solid fa-trophy"></i> <a href="javascript:void(0)" class="award-venue" onclick="openAward('images/awardICRA2025.jpg', 'ICRA 2025 Best Paper Award')">Best Paper Award</a>, ICRA 2025
                        <a href="https://sites.google.com/view/icra-2025-workshop?usp=sharing" style="color: #ffa03c">@ Human-Centric Multilateral Teleoperation </a><i class="fa-solid fa-trophy"></i>
                      </span>
                      <br>
                      <span class="strong-venue">
                        <i class="fa-solid fa-star"></i> Spotlight Presentation, ICRA 2025
                        <a href="https://sites.google.com/view/icra-2025-workshop?usp=sharing" style="color: #66c0ff">@ Human-Centric Multilateral Teleoperation </a><i class="fa-solid fa-star"></i>
                      </span>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Implement a low-cost teleoperation system utilizing data gloves and IMU sensors, paired with an assistant 
                      module that improves data collection process by visualizing future robot operations through visual previews.
                    </div>
                  </td>
                </tr>

                <!-- MetaFold -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/MetaFold.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      MetaFold: A Closed-loop Pipeline for Universal Clothing Folding via End-to-end Point Cloud Trajectory Generation
                    </span>
                    <br>
                
                    <i>
                      <span class="author">Haonan Chen</span>,
                      <span class="author">Junxiao Li</span>,
                      <span class="author">Chongkai Gao</span>,
                      <span class="author">Zhixuan Xu</span>,
                      <span class="author">Chenting Wang</span>,
                      <span class="author">Yiwen Hou</span>,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Shensi Xu</span>,
                      <span class="author">Jiaqi Huang</span>,
                      <span class="author">Weidong Wang</span>,
                      <span class="author">Lin Shao</span>
                    </i>
                    <br>

                    <em>In submission to IROS</em>
                    <br>
    
                    <a href="https://meta-fold.github.io/">Website</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2503.08372">arXiv</a>
                    &nbsp;/&nbsp;
                    Code

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Propose a closed-loop pipeline for universal clothing folding using end-to-end point cloud trajectory generation, 
                      enabling robots to handle various types of clothing with high precision.
                    </div>
                  </td>
                </tr>

                <!-- D(R,O) Grasp -->
                <tr style="background-color: rgba(102, 192, 255, 0.15);">
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/drograsp.png' style="width:100%; height:auto; box-shadow: 0 3px 12px 0 #66c0ff;">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      $\mathcal{D(R,O)}$ Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping
                    </span>
                    <br>

                    <i>
                      <span class="author">Zhenyu Wei</span>*,
                      <span class="author">Zhixuan Xu</span>*,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Yiwen Hou</span>,
                      <span class="author">Chongkai Gao</span>,
                      <span class="author">Zhehao Cai</span>,
                      <span class="author">Jiayu Luo</span>,
                      <span class="author">Lin Shao</span>
                    </i>
                    <br>
                    
                    <a href="https://nus-lins-lab.github.io/drograspweb/">Website</a>
                    &nbsp;/&nbsp;
                    <a href="https://arxiv.org/abs/2410.01702">arXiv</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/zhenyuwei2003/DRO-Grasp">Code</a>
                    &nbsp;/&nbsp;
                    <a href="https://mp.weixin.qq.com/s/csdm7B3G9__QpwEtoIQxuA">Media (Êú∫Âô®‰πãÂøÉ)</a>
                    <br>

                    <div>
                      <!-- ICRA 2025 -->
                      <span class="accepted-venue">ICRA 2025</span>&nbsp;
                      <span class="accepted-venue-detail">International Conference on Robotics and Automation</span>
                      <br>
                      <span class="award-venue">
                        <i class="fa-solid fa-trophy"></i> <a href="javascript:void(0)" onclick="openAward('images/awardICRAbest.jpg', 'ICRA 2025 Best Paper Award')">ICRA 2025 Best Paper Award</a> on Robot Manipulation and Locomotion <i class="fa-solid fa-trophy"></i>
                      </span>
                      <br>
                      <span class="award-venue">
                        <i class="fa-solid fa-trophy"></i>
                        <a href="javascript:void(0)" class="award-venue" onclick="openAward('images/awardcorl2024.jpg', 'D(R,O) Award Image')">Best Robotics Paper Award</a>, CoRL 2024
                        <a href="https://sites.google.com/view/corl-mapodel-workshop/home" style="color: #ffa03c">@ MAPoDeL</a> <i class="fa-solid fa-trophy"></i>
                      </span>
                      <br>
                      <span class="strong-venue">
                        <i class="fa-solid fa-star"></i> Oral Presentation, CoRL 2024 <a href="https://sites.google.com/view/corl-mapodel-workshop/home" style="color: #66c0ff">@ MAPoDeL</a> <i class="fa-solid fa-star"></i>
                      </span>
                      <br>
                      <span class="strong-venue">
                        <i class="fa-solid fa-star"></i> Spotlight Presentation, CoRL 2024 <a href="https://dex-manipulation.github.io/corl2024/index.html" style="color: #66c0ff">@ LFDM</a> <i class="fa-solid fa-star"></i>
                      </span>

                      <div id="AwardModal" class="modal">
                        <div class="modal-content" style="width: 50%; max-width: 800px; margin-top: 8%;">
                          <span class="close" onclick="closeAward()">&times;</span>
                          <img id="awardImage" src="" alt="" style="width: 100%;">
                        </div>
                      </div>
                      
                      <script>
                        function openAward(imagePath, imageAlt) {
                          var modal = document.getElementById("AwardModal");
                          var img = document.getElementById("awardImage");
                          img.src = imagePath;
                          img.alt = imageAlt;
                          modal.style.display = "block";
                        }
                        function closeAward() {
                          document.getElementById("AwardModal").style.display = "none";
                        }
                        window.onload = function() {
                          document.getElementById("AwardModal").style.display = "none";
                        };
                        window.onclick = function(event) {
                          var modal = document.getElementById("AwardModal");
                          if (event.target == modal) {
                            modal.style.display = "none";
                          }
                        }
                      </script>
                    </div>
                    
                    <div class="TLDR">
                      <strong>TL;DR:</strong> 
                      Introduce $\mathcal{D(R,O)}$, a novel interaction-centric representation for dexterous grasping tasks that 
                      goes beyond traditional robot-centric and object-centric approaches, enabling robust generalization across 
                      diverse robotic hands and objects.
                    </div>
                  </td>
                </tr>

                <!-- MASQ -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/MASQ.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>

                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      MASQ: Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion
                    </span>
                    <br>

                    <i>
                      <span class="author">Qi Liu</span>*,
                      <span class="strong-author">Jingxiang Guo</span>*,
                      <span class="author">Sixu Lin</span>,
                      <span class="author">Shuaikang Ma</span>,
                      <span class="author">Jinxuan Zhu</span>,
                      <span class="author">Yanjie Li</span>
                    </i>
                    <br>

                    <em>In submission</em>
                    <br>

                    <a href="https://arxiv.org/abs/2408.13759">arXiv</a>
                    &nbsp;/&nbsp;
                    <a href="https://b23.tv/to312Z4">Video</a>
                    &nbsp;/&nbsp;
                    <a href="https://mp.weixin.qq.com/s/2DKBVJPHFi29DQ46D8YnzA">Press</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong> 
                      Introduce MASQ, a novel approach using multi-agent reinforcement learning (MARL) for single quadruped robot locomotion. By treating each leg as an independent agent, MASQ accelerates learning and boosts real-world robustness, surpassing traditional methods.
                    </div>
                  </td>
                </tr>

                <!-- MARLCC -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/MARLCC.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Multi-Agent Target Assignment and Path Finding for Intelligent Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective
                    </span>
                    <br>
                
                    <i>
                      <span class="author">Qi Liu</span>,
                      <span class="author">Jianqi Gao</span>,
                      <span class="author">Dongjie Zhu</span>,
                      <span class="author">Zhongjian Qiao</span>,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Pengbin Chen</span>,
                      <span class="author">Yanjie Li</span>
                    </i>
                    <br>

                    <em>In submission to IROS</em>
                    <br>
    
                    <a href="https://arxiv.org/abs/2408.13750">arXiv</a>
                    &nbsp;/&nbsp;
                    Code
                    &nbsp;/&nbsp;
                    <a href="https://mp.weixin.qq.com/s/mO17GF53Ilh6HUwMv3Pwdw">Press</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Develop a cooperative multi-agent deep reinforcement learning approach for intelligent warehouse systems, 
                      focusing on efficient target assignment and path finding for multiple robots.
                    </div>
                  </td>
                </tr>

                <!-- logfunction -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/logfunction.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Logarithmic Function Matters Policy Gradient Deep Reinforcement Learning
                    </span>
                    <br>
                
                    <i>
                      <span class="author">Qi Liu</span>,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Zhongjian Qiao</span>,
                      <span class="author">Pengbin Chen</span>,
                      <span class="author">Yanjie Li</span>
                    </i>
                    <br>

                    <!-- DAI 2024 -->
                    <span class="accepted-venue">DAI 2024</span>&nbsp;
                    <span class="accepted-venue-detail">Distributed AI (DAI) conference</span>
                    <br>
                    <span class="strong-venue">
                      <i class="fa-solid fa-star"></i> Oral Presentation <a href="http://www.adai.ai/dai/2024/program-overview.html" style="color: #66c0ff">@ DAI 2024</a> <i class="fa-solid fa-star"></i>
                    </span>
                    <br>
    
                    <a href="https://adai.ai/dai/2024/dai_papers/DAI2024_paper_9.pdf">PDF</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/BorisGuo6/log_function.git">Code</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Investigate the impact of logarithmic functions in policy gradient deep reinforcement learning, 
                      demonstrating improved performance and stability in various RL tasks.
                    </div>
                  </td>
                </tr>

                <!-- LGBM -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/LGBM.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Momentum Prediction for Tennis Matches Based on Counter-Factual Analysis and Multi-LGBM
                    </span>
                    <br>
                
                    <i>
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Jinxuan Zhu</span>,
                      <span class="author">Sixu Lin</span>,
                      <span class="author">Feng Shi</span>
                    </i>
                    <br>
    
                    <a href="https://ieeexplore.ieee.org/document/10548364">IEEE Xplore</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/BorisGuo9/2024_MCM.git">Code</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Develop a novel approach for tennis match momentum prediction using counter-factual analysis and 
                      multi-LGBM models, achieving improved accuracy in match outcome predictions.
                    </div>
                  </td>
                </tr>

                <!-- ECAPA -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/ECAPA.jpg' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      ECAPA-TDNN Embeddings for Speaker Recognition
                    </span>
                    <br>
                
                    <i>
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Jinxuan Zhu</span>,
                      <span class="author">Sixu Lin</span>,
                      <span class="author">Feng Shi</span>
                    </i>
                    <br>
    
                    <a href="https://ieeexplore.ieee.org/document/10581514">IEEE Xplore</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/BorisGuo6/VoxCeleb.git">Code</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Implement and evaluate ECAPA-TDNN embeddings for speaker recognition tasks, 
                      demonstrating improved performance in speaker verification and identification.
                    </div>
                  </td>
                </tr>

                <!-- quickreverse -->
                <tr>
                  <td style="padding:10px; width:30%; vertical-align:middle">
                    <img src='images/quickreverse.png' style="width:100%; height:auto; box-shadow: 0 4px 15px 0 rgba(0, 0, 0, 0.3);">
                  </td>
                
                  <td style="padding:10px; width:70%; vertical-align:middle">
                    <span class="papertitle">
                      Quick reversing device and quick track reversing device
                    </span>
                    <br>
                
                    <i>
                      <span class="author">Kuntian Dai</span>,
                      <span class="strong-author">Jingxiang Guo</span>,
                      <span class="author">Nengfeng Liu</span>,
                      <span class="author">Guanyu Hou</span>,
                      <span class="author">Jinbin Guo</span>,
                      <span class="author">Junkai Wang</span>,
                      <span class="author">Ruiquan Dong</span>
                    </i>
                    <br>

                    <em>National Patent</em>
                    <br>

                    <a href="https://patents.google.com/patent/CN116000896A/en">Google Patents</a>
                    &nbsp;/&nbsp;
                    <a href="javascript:void(0)" class="award-venue" onclick="openCert()">Certificate</a>

                    <div class="TLDR">
                      <strong>TL;DR:</strong>
                      Design and patent a novel quick reversing device and track reversing system, 
                      improving efficiency and safety in industrial applications.
                    </div>
                  </td>
                </tr>
              
                    <!-- Award -->
                    <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-top: 20px; margin-left:auto; margin-right:auto;">
                      <tbody>
                        <tr>
                          <td style="padding-top: 0px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                            <h2><b>Award</b></h2>
                            <ul class="award-list">
                              <li>
                                <span style="font-size: 16px;"><b>Champion</b>, RoboMaster2023 Infantry Match</span>
                                <span style="font-size: 16px;" class="year"><i>Mar 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, RoboMaster2022 (National Final)</span>
                                <span style="font-size: 16px;" class="year"><i>Aug 2022</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, RoboMaster2022 (Southern Region)</span>
                                <span style="font-size: 16px;" class="year"><i>Jun 2022</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, RoboMaster2023 (Sentinel Robot Match)</span>
                                <span style="font-size: 16px;" class="year"><i>Aug 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, RoboMaster2023 (High School League)</span>
                                <span style="font-size: 16px;" class="year"><i>Mar 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, China Intelligent Robots Competition</span>
                                <span style="font-size: 16px;" class="year"><i>Jul 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, Hongli Cup Mathematical Modelling</span>
                                <span style="font-size: 16px;" class="year"><i>Mar 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>First Prize</b>, National Mathematical Modelling, Guangdong Province</span>
                                <span style="font-size: 16px;" class="year"><i>Sep 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Second Prize</b>, National Embedded Chip Design</span>
                                <span style="font-size: 16px;" class="year"><i>Jun 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Second Prize</b>, RoboMaster2022 (National Final)</span>
                                <span style="font-size: 16px;" class="year"><i>Aug 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Second Prize</b>, China Robotics Competition 2023</span>
                                <span style="font-size: 16px;" class="year"><i>Sep 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Second Prize</b>, Mechanical Engineering Innovation 2023</span>
                                <span style="font-size: 16px;" class="year"><i>Sep 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Second Prize</b>, iCAN Innovation and Entrepreneurship</span>
                                <span style="font-size: 16px;" class="year"><i>Nov 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Third Prize</b>, China Robotics AI Competition</span>
                                <span style="font-size: 16px;" class="year"><i>May 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Third Prize</b>, BOTEC Intelligent Robotics Challenge</span>
                                <span style="font-size: 16px;" class="year"><i>Oct 2022</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Third Prize</b>, Harbin Institute Electronic Design</span>
                                <span style="font-size: 16px;" class="year"><i>Apr 2023</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Outstanding Student of the Year</b> 2021-2022</span>
                                <span style="font-size: 16px;" class="year"><i>Nov 2022</i></span>
                              </li>
                              <li>
                                <span style="font-size: 16px;"><b>Outstanding Member of the League</b> 2022-2023</span>
                                <span style="font-size: 16px;" class="year"><i>May 2023</i></span>
                              </li>
                            </ul>
                          </td>
                        </tr>
                      </tbody>
                    </table>



            <!-- Experience -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding-top: 10px; padding-bottom: 10px; padding-left: 20px; padding-right: 20px; width:100%; vertical-align:middle">
                    <h2><b>Experience</b></h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- ScaleLab -->
            <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:20px;">
                    <img src="images/SJTU.png" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      Spatial Cognition and Robotic Automative Learning Laboratory (ScaleLab), China
                      <span class="dates">2024.05 - Present</span>
                    </h3>
                    <b>Research Intern</b>
                    <br>
                    Advisor: Prof. <a href="https://yaomarkmu.github.io/">Yao Mu</a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- LinS Lab -->
            <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:20px;">
                    <img src="images/linslab.jpg" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      NUS Learning and Intelligent Systems Lab (LinS Lab), Singapore
                      <span class="dates">2024.07 - 2025.5</span>
                    </h3>
                    <b>Research Intern</b>
                    <br>
                    Advisor: Prof. <a href="https://linsats.github.io/">Lin Shao</a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- NUS -->
            <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:20px;">
                    <img src="images/NUS.png" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      National University of Singapore, Singapore
                      <span class="dates">2024.07 - 2025.05</span>
                    </h3>
                    NGNE Program Exchange Student
                    <br>
                    <b>GPA:</b> 4.2/5.0
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- RLG -->
            <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:20px;">
                    <img src="images/RLG.png" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      HTISZ Reinforcement Learning Group (RLG), Shenzhen, China
                      <span class="dates">2022.10 - 2024.06</span>
                    </h3>
                    <b>Research Intern</b>
                    <br>
                    Advisor: Prof. <a href="https://lktunnel.ipyingshe.net/base/">Yanjie Li</a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- HITSZ -->
            <table style="width:98%; margin-left:20px; border:none; padding:0px;">
              <tbody>
                <tr>
                  <td style="padding:25px; width:15%; vertical-align:middle; padding-top:20px;">
                    <img src="images/HIT.png" style="width:100%; height:auto;">
                  </td>
                  <td style="padding-top:20px; padding-left:20px; padding-right:20px; width:85%; vertical-align:top;">
                    <h3>
                      Harbin Institute of Technology, Shenzhen, China
                      <span class="dates">2021.09 - 2025.07</span>
                    </h3>
                    B.E. in Automation
                    <br>
                    <b>GPA:</b> 3.7/4.0
                  </td>
                </tr>
              </tbody>
            </table>

            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=GgviwSm_ICUFU9G8Aqqxy1gGzImeAwMwS4JaFgIXMQQ&cl=ffffff&w=a"></script>

            <!-- Footnote -->
            <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-left:auto; margin-right:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right; font-size:small;">
                      Thanks for your visitingüòä! Feel free to contact me if you have any problems.
                      <br>
                      This website is designed based on <a href="https://jonbarron.info/" style="font-size: 12px;">Jon Barron</a> and <a href="https://zhenyuwei2003.github.io/" style="font-size: 12px;">Zhenyu Wei</a>.
                      Last Update: May 24, 2024
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
